{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AML SDK version: 1.42.0\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n",
      "\n",
      "Running\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "from azureml.core import (\n",
    "    Workspace\n",
    "    , Experiment\n",
    "    , Environment\n",
    "    , Dataset\n",
    "    , RunConfiguration\n",
    ")\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.runconfig import (\n",
    "    RunConfiguration\n",
    "    , DEFAULT_CPU_IMAGE\n",
    "    , DockerConfiguration\n",
    ")\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "from azureml.pipeline.core import (\n",
    "    Pipeline\n",
    "    , PipelineRun\n",
    "    , PipelineParameter\n",
    "    , PipelineData\n",
    "    , TrainingOutput\n",
    ")\n",
    "from azureml.pipeline.steps import (\n",
    "    PythonScriptStep\n",
    "    , HyperDriveStep\n",
    ")\n",
    "\n",
    "# from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "\n",
    "print(\"AML SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "# Load the workspace from a configuration file\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get a reference to our experiment\n",
    "exp = Experiment(ws, 'sklearn_pipeline_exp')\n",
    "\n",
    "# Build environment\n",
    "env = Environment.from_conda_specification(name='test_env', file_path=\"conda_dependencies.yml\")\n",
    "\n",
    "# Set the compute target  and compute cluster\n",
    "cluster = ComputeTarget(workspace=ws, name='cpu-cluster')\n",
    "cluster.wait_for_completion(show_output=True)\n",
    "compute = ComputeTarget(workspace=ws, name='crcastillo841')\n",
    "compute.wait_for_completion(show_output=True)\n",
    "\n",
    "# Establish default datastore\n",
    "default_store = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the run configuration\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "run_config.docker = DockerConfiguration(use_docker=True)\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify conda dependencies through .yml\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies(conda_dependencies_file_path=\"conda_dependencies.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prep step is created\n"
     ]
    }
   ],
   "source": [
    "# Define dataset\n",
    "ds_input = Dataset.get_by_name(\n",
    "        workspace=ws\n",
    "        , name=\"1994_Adult_Data\"\n",
    "    ).as_named_input('ds_input')\n",
    "\n",
    "# Define outputs\n",
    "train_data = PipelineData('train_data', datastore=default_store).as_dataset()\n",
    "test_data = PipelineData('test_data', datastore=default_store).as_dataset()\n",
    "\n",
    "# Define variables\n",
    "random_seed = 123\n",
    "test_proportion = 0.2\n",
    "target = 'income'\n",
    "\n",
    "# Create prep_step\n",
    "prep_step = PythonScriptStep(\n",
    "    source_directory='./prep'\n",
    "    , script_name='prep.py'\n",
    "    , arguments=[\n",
    "        '--random_seed', random_seed\n",
    "        , '--test_proportion', test_proportion\n",
    "        , '--target', target\n",
    "        , '--train_data', train_data\n",
    "        , '--test_data', test_data\n",
    "        ]\n",
    "    , inputs=[ds_input]\n",
    "    , outputs=[\n",
    "        train_data\n",
    "        , test_data\n",
    "        ]\n",
    "    , runconfig=run_config\n",
    "    , compute_target=compute\n",
    "    , allow_reuse=True\n",
    "    )\n",
    "\n",
    "# Print\n",
    "print('prep step is created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define outputs\n",
    "metrics_data = PipelineData(\n",
    "    name='metrics_data'\n",
    "    , datastore=default_store\n",
    "    , pipeline_output_name='metrics_output'\n",
    "    , training_output=TrainingOutput(type='Metrics')\n",
    ")\n",
    "model_data = PipelineData(\n",
    "    name='model_data'\n",
    "    , datastore=default_store\n",
    "    , pipeline_output_name='metrics_output'\n",
    "    , training_output=TrainingOutput(\n",
    "        type='Model'\n",
    "        , model_file=\"outputs/model.pkl\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define search space and parameter search\n",
    "param_sampling = BayesianParameterSampling({\n",
    "    \"l1_ratio\": uniform(\n",
    "        min_value=0\n",
    "        , max_value=1\n",
    "        ),\n",
    "    \"C\": choice(\n",
    "        [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2]\n",
    "        )\n",
    "})\n",
    "\n",
    "# Create hyperdrive config\n",
    "hd_config = HyperDriveConfig(\n",
    "    run_config=run_config\n",
    "    , hyperparameter_sampling=param_sampling\n",
    "    , policy=None\n",
    "    , primary_metric_name='mean_cv_score'\n",
    "    , primary_metric_goal=PrimaryMetricGoal.MAXIMIZE\n",
    "    , max_total_runs=40\n",
    "    , max_concurrent_runs=4\n",
    "    )\n",
    "\n",
    "# Create hyperdrive_step\n",
    "hypedrive_step = HyperDriveStep(\n",
    "    name='hyperdrive_step'\n",
    "    , hyperdrive_config=\n",
    "    , source_directory='./train'\n",
    "    , script_name='train.py'\n",
    "    , arguments=[\n",
    "        '--random_seed', random_seed\n",
    "        , '--train_data', train_data\n",
    "        ]\n",
    "    , inputs=[train_data]\n",
    "    , outputs=[model]\n",
    "    , runconfig=run_config\n",
    "    , compute_target=compute\n",
    "    , allow_reuse=True\n",
    "    )\n",
    "\n",
    "# Print\n",
    "print('hyperdrive step is created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step prep.py is ready to be created [b14bed48]\n",
      "Created step prep.py [b14bed48][66d6224a-dda3-41b1-8eeb-41a6a5b7b901], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 452481a2-bbb6-4e16-9500-7b3aa8a371b9\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/452481a2-bbb6-4e16-9500-7b3aa8a371b9?wsid=/subscriptions/8841158c-7729-48e1-a4e3-b8125457e298/resourcegroups/azure_ml/workspaces/azure_ml_poc&tid=1dfa1b70-a4aa-42af-91e8-993521be798f\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>sklearn_poc_pipeline</td><td>452481a2-bbb6-4e16-9500-7b3aa8a371b9</td><td>azureml.PipelineRun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/452481a2-bbb6-4e16-9500-7b3aa8a371b9?wsid=/subscriptions/8841158c-7729-48e1-a4e3-b8125457e298/resourcegroups/azure_ml/workspaces/azure_ml_poc&amp;tid=1dfa1b70-a4aa-42af-91e8-993521be798f\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: sklearn_poc_pipeline,\n",
       "Id: 452481a2-bbb6-4e16-9500-7b3aa8a371b9,\n",
       "Type: azureml.PipelineRun,\n",
       "Status: Preparing)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    workspace=ws\n",
    "    , steps=[\n",
    "        prep_step\n",
    "        , train_step\n",
    "        # , score_step\n",
    "        ]\n",
    ")\n",
    "pipeline.validate()\n",
    "pipeline.submit('sklearn_poc_pipeline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('azureml_py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
