{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset, Workspace\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom transformers to use within sklearn pipelines\n",
    "\n",
    "# Load required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import (\n",
    "    BaseEstimator\n",
    "    , TransformerMixin\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Create custom transformer that converts to defined type\n",
    "class TypeConversion_Manual(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self, column_type_dict: dict):\n",
    "        self.column_type_dict = column_type_dict\n",
    "    # Return self\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y=None):\n",
    "        # Create a copy of the dataframe\n",
    "        X_Copy = X.copy()\n",
    "        # Iterate through column_type_dict to convert fields to defined astype\n",
    "        for i in self.column_type_dict:\n",
    "            X_Copy[i] = X_Copy[i].astype(self.column_type_dict[i])\n",
    "        # Return the copied DataFrame with the fixed field\n",
    "        return X_Copy\n",
    "\n",
    "\n",
    "# Create custom transformer that converts to str\n",
    "class ObjectConversion_Manual(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self, Object_List: list):\n",
    "        self.Object_List = Object_List\n",
    "    # Return self\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y=None):\n",
    "        # Create a copy of the dataframe\n",
    "        X_Copy = X.copy()\n",
    "        # Iterate through Object_List to convert fields to str then np.object\n",
    "        for i in self.Object_List:\n",
    "            Store = np.where(\n",
    "                pd.isnull(X_Copy[i])\n",
    "                , X_Copy[i]\n",
    "                , X_Copy[i].astype(str)\n",
    "                ).copy()\n",
    "            X_Copy[i] = Store\n",
    "        # Return the copied DataFrame with the fixed field\n",
    "        return X_Copy\n",
    "\n",
    "# Create custom transformer that drops features with a percent missing exceeding a defined cutoff\n",
    "class FeatureMissingness(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self, cutoff: float):\n",
    "        self.cutoff = cutoff\n",
    "    # Return self\n",
    "    def fit(self, X, y = None):\n",
    "        # Create a DataFrame with the percent missing for each field\n",
    "        self.X_Missing = pd.DataFrame(X.isnull().sum().sort_values(ascending=False) / X.shape[0])\n",
    "        # Identify the fields to drop\n",
    "        self.X_Missing_Fields = self.X_Missing[self.X_Missing[0] > self.cutoff].index.to_list()\n",
    "\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y = None):\n",
    "        # Return the copied DataFrame\n",
    "        return X.loc[:, ~X.columns.isin(self.X_Missing_Fields)]\n",
    "\n",
    "# Create custom transformer that coerces categorical levels to np.nan per a user defined dictionary key-value pairing\n",
    "class NA_Manual(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self, NA_Manual_Dict: dict):\n",
    "        self.NA_Manual_Dict = NA_Manual_Dict\n",
    "    # Return self\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y = None):\n",
    "        # Create a copy of the dataframe\n",
    "        X_Copy = X.copy()\n",
    "        # Iterate through NA_Manual_Dic to coerce specified categorical levels to np.nan\n",
    "        for i in self.NA_Manual_Dict:\n",
    "            X_Copy.loc[X_Copy[i].isin(self.NA_Manual_Dict[i]), i] = np.nan\n",
    "        # Return the copied DataFrame with the fixed field\n",
    "        return X_Copy\n",
    "\n",
    "# Create custom transformer that fills NaN fields per a user defined dictionary key-value pairing\n",
    "class Fill_NA_Manual(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self, Fill_NA_Dict: dict):\n",
    "        self.Fill_NA_Dict = Fill_NA_Dict\n",
    "    # Return self\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y = None):\n",
    "        # Create a copy of the dataframe\n",
    "        X_Copy = X.copy()\n",
    "        # Iterate through Fill_NA_Dict to fillna with desired value\n",
    "        for i in self.Fill_NA_Dict:\n",
    "            Store = X_Copy[i].fillna(self.Fill_NA_Dict[i]).copy()\n",
    "            X_Copy[i] = Store\n",
    "        # Return the copied DataFrame with the fixed field\n",
    "        return X_Copy\n",
    "\n",
    "# Create custom transformer that drops defined features\n",
    "class HardExclude(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self, Exclude_Fields: list):\n",
    "        self.Exclude_Fields = Exclude_Fields\n",
    "    # Return self\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y = None):\n",
    "        # Create copy of DF while excluding the defined fields\n",
    "        X_Copy = X.loc[:, ~X.columns.isin(self.Exclude_Fields)].copy()\n",
    "        # Return the copied DataFrame\n",
    "        return X_Copy\n",
    "\n",
    "# Create custom transformer that collapses low observation categories into _Other_ based on a user provided dictionary\n",
    "class LowObsCounts_Manual(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self, Fill_Other_Dict: dict):\n",
    "        self.Fill_Other_Dict = Fill_Other_Dict\n",
    "    # Return self\n",
    "    def fit(self, X, y = None):\n",
    "        # Instantiate dictionary to map category transforms\n",
    "        self.Transform_Dict = {}\n",
    "        # Iterate through Fill_Other_Dict to coerce categories below specified threshold with _Other_\n",
    "        for i in self.Fill_Other_Dict:\n",
    "            # Store Series object with proportions of non-na levels\n",
    "            Low_Obs_Column = X[i].value_counts().sort_values(ascending=True) / X[i].count()\n",
    "            # Store low observation categories\n",
    "            Low_Obs_Categories = Low_Obs_Column.index[Low_Obs_Column < self.Fill_Other_Dict[i]].to_list()\n",
    "            # Populate the Transform_Dict with the categories for each variable\n",
    "            self.Transform_Dict[i] = Low_Obs_Categories\n",
    "\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y = None):\n",
    "        # Create a copy of the dataframe\n",
    "        X_Copy = X.copy()\n",
    "        # Iterate through Fill_Other_Dict to coerce categories below specified threshold with _Other_\n",
    "        for Iter in self.Transform_Dict:\n",
    "            # Replace low observation categories with _Other_\n",
    "            X_Copy.loc[X_Copy[Iter].isin(self.Transform_Dict[Iter]), Iter] = '_Other_'\n",
    "        # Return the copied DataFrame with the fixed field\n",
    "        return X_Copy\n",
    "\n",
    "# Create custom transformer that transforms defined variables with log(x + 1)\n",
    "class LogPlusOne_Manual(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self, Transform_Fields: list):\n",
    "        self.Transform_Fields = Transform_Fields\n",
    "    # Return self\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y = None):\n",
    "        # Create a copy of the dataframe\n",
    "        X_Copy = X.copy()\n",
    "        # Define transformation function\n",
    "        def LogPlusOne_Transform(x):\n",
    "            if pd.isna(x):\n",
    "                return np.nan\n",
    "            elif x > 0:\n",
    "                return np.log(x + 1)\n",
    "            else:\n",
    "                return np.log(1)\n",
    "        # Iterate through Fill_Other_Dict to coerce categories below specified threshold with _Other_\n",
    "        for i in self.Transform_Fields:\n",
    "            # Store a copy of the field and apply the transform\n",
    "            Transform_Field = X_Copy[i].apply(LogPlusOne_Transform).copy()\n",
    "            # Overwrite the transformed field\n",
    "            X_Copy[i] = Transform_Field\n",
    "        # Return the copied DataFrame\n",
    "        return X_Copy\n",
    "\n",
    "# Create custom transformer that identifies correlation pairs above threshold and removes one with highest mean abs corr\n",
    "class FindCorrelation(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self, threshold = 0.95):\n",
    "        self.threshold = threshold\n",
    "    # Return self\n",
    "    def fit(self, X, y = None):\n",
    "        # Store matrix of correlation coefficients\n",
    "        self.corr_mat = np.corrcoef(X.T)\n",
    "        # Store absolute value correlation matrix\n",
    "        abs_corr_mat = np.abs(self.corr_mat)\n",
    "        # Store the lower diagonal\n",
    "        self.corr_mat = np.tril(m = self.corr_mat, k = -1)\n",
    "        # Instantiate the array to store columns to remove\n",
    "        self.Remove = []\n",
    "        # Iterate through columns of input matrix to check for pairs above threshold\n",
    "        for Col in range(X.shape[1]):\n",
    "            # Store the row element that exceeds threshold\n",
    "            Row = np.where(np.abs(self.corr_mat[:, Col]) > self.threshold)[0]\n",
    "            if len(Row) > 0:\n",
    "                # Append Row to Remove with lowest mean corr\n",
    "                if np.mean(abs_corr_mat[:, Col]) > np.mean(abs_corr_mat[Row[0]]):\n",
    "                    self.Remove.append(Row[0])\n",
    "                else:\n",
    "                    self.Remove.append(Col)\n",
    "        # Eliminate duplicates\n",
    "        self.Remove = np.unique(self.Remove)\n",
    "        # Store vector of False booleans\n",
    "        Remove_Bool = np.zeros(shape = X.shape[1], dtype = bool)\n",
    "        # Check to see if Remove is empty\n",
    "        if len(self.Remove) != 0:\n",
    "            # Use self.Remove to switch corresponding elements to True\n",
    "            Remove_Bool[self.Remove] = True\n",
    "        # Store final vector of columns to remove\n",
    "        self.Remove_Vector = Remove_Bool\n",
    "\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y = None):\n",
    "        # Create a copy of the matrix without highly correlated columns\n",
    "        return X.T[~self.Remove_Vector].T\n",
    "\n",
    "# Create custom transformer that converts all object fields to category fields\n",
    "class CategoryConverter(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self):\n",
    "        self\n",
    "    # Return self\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y = None):\n",
    "        # Create a copy of the dataframe\n",
    "        X_Copy = X.copy()\n",
    "        # Iterate through Fill_Other_Dict to coerce categories below specified threshold with _Other_\n",
    "        for i in X_Copy.select_dtypes('O').columns:\n",
    "            # Store a copy of the field and apply the transform to category\n",
    "            Transform_Field = X_Copy[i].astype('category').cat.remove_unused_categories().copy()\n",
    "            # Overwrite the transformed field\n",
    "            X_Copy[i] = Transform_Field\n",
    "        # Return the copied DataFrame\n",
    "        return X_Copy\n",
    "\n",
    "# Create custom transformer that coerces all new categorical levels to NaN\n",
    "class NewCategoryLevels(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self):\n",
    "        self\n",
    "    # Return self\n",
    "    def fit(self, X, y = None):\n",
    "        # Instantiate a dictionary to store categorical levels\n",
    "        self.category_levels = {}\n",
    "        # Identify category fields and store their respective levels\n",
    "        for i in X.select_dtypes('category').columns:\n",
    "            self.category_levels[i] = X[i].cat._parent._dtype._categories\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y = None):\n",
    "        # Create a copy of the dataframe\n",
    "        X_Copy = X.copy()\n",
    "        # Iterate through self.category_levels to set levels accordingly and coerce new levels to NaN\n",
    "        for i in self.category_levels.keys():\n",
    "            # Store a copy of the field and set the category levels according to fit\n",
    "            Transform_Field = X_Copy[i].cat.set_categories(self.category_levels[i])\n",
    "            # Overwrite the transformed field\n",
    "            X_Copy[i] = Transform_Field\n",
    "        # Return the copied DataFrame\n",
    "        return X_Copy\n",
    "\n",
    "# Create custom transformer that identifies correlation pairs above threshold and removes one with highest mean abs corr\n",
    "class VIFScreen(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor\n",
    "    def __init__(self, threshold = 10.0):\n",
    "        self.threshold = threshold\n",
    "    # Return self\n",
    "    def fit(self, X, y = None):\n",
    "        # Retrieve numeric data from X and assign a Constant field\n",
    "        Numeric_Data = pd.DataFrame(\n",
    "            data = X\n",
    "            , dtype='float64'\n",
    "        ).assign(Constant = 1)\n",
    "        # Store VIF data\n",
    "        self.VIF = pd.Series(\n",
    "            data = [variance_inflation_factor(Numeric_Data.values, i)\n",
    "                   for i in range(Numeric_Data.shape[1])]\n",
    "            , index = Numeric_Data.columns\n",
    "        ).sort_values(ascending = False)\n",
    "        # Drop the constant field\n",
    "        self.VIF.drop(labels = 'Constant', inplace = True)\n",
    "        # Store the columns to remove\n",
    "        self.Remove = self.VIF[self.VIF > self.threshold].index.to_list()\n",
    "        # Store vector of False booleans\n",
    "        Remove_Bool = np.zeros(shape = X.shape[1], dtype = bool)\n",
    "        # Check to see if Remove is empty\n",
    "        if len(self.Remove) != 0:\n",
    "            # Use self.Remove to switch corresponding elements to True\n",
    "            Remove_Bool[self.Remove] = True\n",
    "        # Store final vector of columns to remove\n",
    "        self.Remove_Vector = Remove_Bool\n",
    "\n",
    "        return self\n",
    "    # Method that describes what we need the transformer to do\n",
    "    def transform(self, X, y = None):\n",
    "        # Create a copy of the matrix without highly correlated columns\n",
    "        return X.T[~self.Remove_Vector].T\n",
    "\n",
    "class CategoricalImputer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, missing_values=np.nan, strategy='most_frequent', fill_value=None):\n",
    "        self.missing_values = missing_values\n",
    "        self.strategy = strategy\n",
    "        self.fill_value = fill_value\n",
    "\n",
    "        # Check for allowed strategies\n",
    "        allowed_strategies = [\"most_frequent\", \"constant\"]\n",
    "        if self.strategy not in allowed_strategies:\n",
    "            raise ValueError(\"Can only use these strategies: {0} \"\n",
    "                             \" got strategy={1}\".format(allowed_strategies, self.strategy))\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        # Instantiate a dictionary to store categorical columns\n",
    "        self.category_columns = {}\n",
    "        # Identify category fields and store their respective levels\n",
    "        if self.strategy == 'most_frequent':\n",
    "            for i in X.columns:\n",
    "                self.category_columns[i] = X[i].mode(dropna=True)[0]\n",
    "        elif self.strategy == 'constant':\n",
    "            for i in X.columns:\n",
    "                self.category_columns[i] = self.fill_value\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        # Create a copy of the dataframe\n",
    "        X_Copy = X.copy()\n",
    "        # Iterate through self.category_levels to set levels accordingly and coerce new levels to NaN\n",
    "        for i in self.category_columns.keys():\n",
    "            # Store a copy of the field and set the category levels according to fit\n",
    "            Transform_Field = X_Copy[i].fillna(value=self.category_columns[i])\n",
    "            # Overwrite the transformed field\n",
    "            X_Copy[i] = Transform_Field\n",
    "        # Return the copied DataFrame\n",
    "        return X_Copy\n",
    "\n",
    "# Create custom transformer that speeds up the most_frequent imputation for categorical variables\n",
    "class IndicateMissing(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, missing_values=np.nan):\n",
    "        self.missing_values = missing_values\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        # Instantiate list to store columns that contain NaN\n",
    "        self.NaN_Columns = []\n",
    "        # Iterate through all columns looking for ones that contain a NaN (or NA)\n",
    "        for col in X.columns:\n",
    "            if X[col].isna().sum() > 0:\n",
    "                self.NaN_Columns.append(col)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        # Create a copy of the dataframe\n",
    "        X_Copy = X.copy()\n",
    "        # Iterate through self.category_levels to set levels accordingly and coerce new levels to NaN\n",
    "        for col in self.NaN_Columns:\n",
    "            # Store a copy of the field indicating which records are NaN set to 'category' type\n",
    "            NaN_Check = X_Copy[col].isna().astype('category')\n",
    "            # Create new field\n",
    "            X_Copy[str(col + '_NaN')] = NaN_Check\n",
    "        # Return the copied DataFrame\n",
    "        return X_Copy\n",
    "\n",
    "# Create custom transformer that can perform label encoding and store the classes in a callable dictionary\n",
    "class ModifiedLabelEncoder(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        # Check to ensure data is in DataFrame\n",
    "        X_df = pd.DataFrame(X)\n",
    "        # Instantiate dictionary to store label encoders that we'll fit\n",
    "        self.labels_dict = {}\n",
    "        # Iterate through all columns looking for ones that contain a NaN (or NA)\n",
    "        for col in X_df.columns:\n",
    "            self.labels_dict[col] = LabelEncoder().fit(X_df[col])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        # Create a copy of the dataframe\n",
    "        X_Copy = pd.DataFrame(X.copy())\n",
    "        # Iterate through self.category_levels to set levels accordingly and coerce new levels to NaN\n",
    "        for key in self.labels_dict.keys():\n",
    "            # Store a copy of the field and set the category levels according to fit\n",
    "            Transform_Field = self.labels_dict[key].transform(X_Copy[key])\n",
    "            # Overwrite the transformed field\n",
    "            X_Copy[key] = Transform_Field\n",
    "        # Return the copied DataFrame\n",
    "        return X_Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler\n",
    "    , OneHotEncoder\n",
    ")\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer\n",
    "    , make_column_selector\n",
    ")\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# This will allow a new Pipeline object to be instantiated without persisting connections to the underlying object\n",
    "def CreateProcessingPipeline(verbose = False):\n",
    "    # Define Numeric transformations\n",
    "    Numeric_Transformer = Pipeline(\n",
    "        steps=[\n",
    "            ('SimpleImputer', SimpleImputer(\n",
    "                strategy='median'\n",
    "                , verbose=1))\n",
    "            # No need for single numeric variable | commenting out FindCorrelation and VIFScreen\n",
    "            # , ('CorrelationCheck', FindCorrelation(threshold=0.95))            \n",
    "            # , ('Run VIF screen' ,VIFScreen(threshold = 10.0))\n",
    "            , ('StandardScaler', StandardScaler())\n",
    "        ]\n",
    "        , verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Define Numeric transformations\n",
    "    Categorical_Transformer = Pipeline(\n",
    "        steps=[\n",
    "            ('CategoricalImputer', CategoricalImputer(\n",
    "                strategy='most_frequent'))\n",
    "            , ('LabelEncoder', ModifiedLabelEncoder())\n",
    "            , ('OneHotEncoding', OneHotEncoder(\n",
    "                handle_unknown='error'\n",
    "                , drop='first'\n",
    "                , sparse=False))\n",
    "        ]\n",
    "        , verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Combine the Numeric and Categorical transformers\n",
    "    Combined_Transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('NumericTransforms'\n",
    "             , Numeric_Transformer\n",
    "             , make_column_selector(dtype_include=np.number))\n",
    "            , ('CategoricalTransforms'\n",
    "               , Categorical_Transformer\n",
    "               , make_column_selector(dtype_include='category'))\n",
    "        ]\n",
    "        , remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Inherit the Pipeline_PreProcessing object\n",
    "    Processing_Pipeline = Pipeline(\n",
    "        steps = [\n",
    "            ('Convert_Column_Types', TypeConversion_Manual(column_type_dict={}))\n",
    "            , ('Remove_High_Missing', FeatureMissingness(cutoff=0.90))\n",
    "            , ('Convert_Object_To_Category', CategoryConverter())\n",
    "            , ('Coerce_Novel_Levels', NewCategoryLevels())\n",
    "            , ('Combined_Transforms', Combined_Transformer)\n",
    "            , ('Variance_Screen', VarianceThreshold(threshold = 0))\n",
    "        ]\n",
    "        , verbose=verbose\n",
    "    )\n",
    "\n",
    "    # Return the final object\n",
    "    return Processing_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get the input dataset by ID\n",
    "dataset = Dataset.get_by_name(\n",
    "    workspace=ws\n",
    "    , name=\"1987_NICP_Survey\"\n",
    ")\n",
    "\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "raw_df = dataset.to_pandas_dataframe()\n",
    "\n",
    "# Split Train_Data into Train/Valid\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(\n",
    "    raw_df.loc[:, ~raw_df.columns.isin(['ContraceptiveMethod'])]\n",
    "    , raw_df.loc[: , raw_df.columns.isin(['ContraceptiveMethod'])]\n",
    "    , random_state=123\n",
    "    , test_size=0.2\n",
    ")\n",
    "\n",
    "# Fix Y_Train and Y_Test\n",
    "Y_Train.replace(\n",
    "    to_replace={3:1}\n",
    "    , inplace = True\n",
    ")\n",
    "\n",
    "Y_Test.replace(\n",
    "    to_replace={3:1}\n",
    "    , inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1178, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the PreProcessing Pipeline\n",
    "PreProcessing_Pipeline = CreateProcessingPipeline(verbose=False)\n",
    "\n",
    "# Create dictionary of how columns types shoudl be transformed\n",
    "To_Type_Dict = {\n",
    "    'WifesAge': 'Int64'\n",
    "    , 'WifesEducation': object\n",
    "    , 'HusbandsEducation': object\n",
    "    , 'NumberOfChildren': object\n",
    "    , 'WifesReligion': object\n",
    "    , 'WifeWorking': object \n",
    "    , 'HusbandOccupation': object\n",
    "    , 'StandardOfLivingIndex': object \n",
    "    , 'MediaExposure': object \n",
    "    # Comment out the Target\n",
    "    # , 'ContraceptiveMethod': object\n",
    "}\n",
    "\n",
    "# Alter the parameters of pipeline\n",
    "PreProcessing_Pipeline.set_params(**{\n",
    "    'Convert_Column_Types__column_type_dict': To_Type_Dict\n",
    "})\n",
    "\n",
    "# Transform X_Train for model fitting\n",
    "X_Train_trans = PreProcessing_Pipeline.fit_transform(X=X_Train)\n",
    "\n",
    "# Transform X_Test\n",
    "X_Test_trans = PreProcessing_Pipeline.transform(X=X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1178, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_Train_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1178, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1178, 31)\n",
      "(295, 31)\n"
     ]
    }
   ],
   "source": [
    "train = np.column_stack((Y_Train, X_Train_trans))\n",
    "test = np.column_stack((Y_Test, X_Test_trans))\n",
    "\n",
    "print(np.shape(train))\n",
    "print(np.shape(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29576759890258475\n",
      "[0.2957676 0.        0.        1.        0.        0.        1.\n",
      " 0.        0.        0.        1.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(X_Train_trans[0, 0])\n",
    "\n",
    "print(X_Train_trans[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('azureml_py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
